{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quran2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aSafarpoor/NLP-hw-Quran/blob/main/Quran2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97JFHP18d-FG"
      },
      "source": [
        "#Alireza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geQ4K2taPMQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45c6852-d6e0-4bed-a51c-d38c59242bac"
      },
      "source": [
        "% pip install pyarabic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (0.6.14)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFDsQBHQPS_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c560751c-fbf0-4792-9ae9-7fb96192dd1f"
      },
      "source": [
        "%pip install camel_tools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: camel_tools in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.10.0+cu111)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from camel_tools) (4.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.6.2)\n",
            "Requirement already satisfied: camel-kenlm in /usr/local/lib/python3.7/dist-packages (from camel_tools) (2020.11.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.0.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.3.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.5.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from camel_tools) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.19.5)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from camel_tools) (4.2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from camel_tools) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from camel_tools) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->camel_tools) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (4.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (0.1.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel_tools) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.0.2->camel_tools) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.2->camel_tools) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->camel_tools) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel_tools) (2.8.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->camel_tools) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->camel_tools) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->camel_tools) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->camel_tools) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->camel_tools) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->camel_tools) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->camel_tools) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gizro0GQPJPm"
      },
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import requests\n",
        "import codecs\n",
        "import pyarabic.araby as araby\n",
        "from camel_tools.utils.charmap import CharMapper\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NldCBSUrRUK1"
      },
      "source": [
        "# alireza code ... now commented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjNzSk10LbFV"
      },
      "source": [
        "# import xml.etree.ElementTree as et\n",
        "# import codecs\n",
        "\n",
        "# xtree = et.parse(R\"Quran Data/Quran_Words.xml\")\n",
        "# xroot = xtree.getroot()\n",
        "# quran = []\n",
        "# i = 0\n",
        "# while i<len(xroot):\n",
        "#     sureh_num = xroot[i].attrib['sureh']\n",
        "#     aye_num = xroot[i].attrib['aye']\n",
        "#     if aye_num==\"1\" and sureh_num!=\"1\" and sureh_num!=\"9\":\n",
        "#         i=i+1\n",
        "#         continue\n",
        "\n",
        "#     verse = []\n",
        "#     while True:\n",
        "#         if i<len(xroot) and xroot[i].attrib['aye'] == aye_num:\n",
        "#             verse.append(xroot[i].attrib['entry'])\n",
        "#             i = i+1\n",
        "#         else:\n",
        "#             break\n",
        "#     sureh_num = int(sureh_num)\n",
        "#     aye_num = int(aye_num)\n",
        "#     if sureh_num != 1 and sureh_num != 9:\n",
        "#         aye_num = aye_num - 1\n",
        "#     id = F\"{sureh_num}##{aye_num}\"\n",
        "#     quran.append(F\"{id}\\t{' '.join(verse)}\")\n",
        "\n",
        "# with codecs.open(\"output/v_seperated.txt\", 'w', 'utf-8') as f:\n",
        "#     for line in quran:\n",
        "#         f.write(line+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maWZVI7AOmqv"
      },
      "source": [
        "reading Alireza output text from github and make it ready on Quran list:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fghgPuTAQSZ9"
      },
      "source": [
        "# from internet, copied to change if necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyOEBYEgMdoD"
      },
      "source": [
        "\n",
        "aa = \"َ\"\n",
        "ee = \"ِ\"\n",
        "oo = \"ُ\"\n",
        "_ALEF_NORMALIZE_BW_RE = re.compile(u'[<>{|]')\n",
        "_ALEF_NORMALIZE_SAFEBW_RE = re.compile(u'[IOLM]')\n",
        "_ALEF_NORMALIZE_XMLBW_RE = re.compile(u'[IO{|]')\n",
        "_ALEF_NORMALIZE_HSB_RE = re.compile(u'[\\u0102\\u00c2\\u00c4\\u0100]')\n",
        "_ALEF_NORMALIZE_AR_RE = re.compile(u'[\\u0625\\u0623\\u0671\\u0622]')\n",
        "_UNICODE_CHAR_FIX = CharMapper({\n",
        "                        '\\ufdfc': 'ريال',\n",
        "                        '\\ufdfd': 'بسم الله الرحمن الرحيم',\n",
        "                    })\n",
        "def normalize_unicode(s, compatibility=True):\n",
        "    \"\"\"Normalize Unicode strings into their canonically composed form or\n",
        "    (i.e. characters that can be written as a combination of unicode characters\n",
        "    are converted to their single character form).\n",
        "\n",
        "    Note: This is essentially a call to :func:`unicodedata.normalize` with\n",
        "    form 'NFC' if **compatibility** is False or 'NFKC' if it's True.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "        compatibility (:obj:`bool`, optional): Apply compatibility\n",
        "            decomposition. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    if compatibility:\n",
        "        fixed = _UNICODE_CHAR_FIX(s)\n",
        "        return unicodedata.normalize('NFKC', fixed)\n",
        "\n",
        "    return unicodedata.normalize('NFC', s)\n",
        "def normalize_alef_maksura_bw(s):\n",
        "    \"\"\"Normalize all occurences of Alef Maksura characters to a Yeh character\n",
        "    in a Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'Y', u'y')\n",
        "def normalize_alef_maksura_safebw(s):\n",
        "    \"\"\"Normalize all occurences of Alef Maksura characters to a Yeh character\n",
        "    in a Safe Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'Y', u'y')\n",
        "def normalize_alef_maksura_xmlbw(s):\n",
        "    \"\"\"Normalize all occurences of Alef Maksura characters to a Yeh character\n",
        "    in a XML Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'Y', u'y')\n",
        "def normalize_alef_maksura_hsb(s):\n",
        "    \"\"\"Normalize all occurences of Alef Maksura characters to a Yeh character\n",
        "    in a Habash-Soudi-Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'\\u00fd', u'y')\n",
        "def normalize_alef_maksura_ar(s):\n",
        "    \"\"\"Normalize all occurences of Alef Maksura characters to a Yeh character\n",
        "    in an Arabic string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'\\u0649', u'\\u064a')\n",
        "def normalize_teh_marbuta_bw(s):\n",
        "    \"\"\"Normalize all occurences of Teh Marbuta characters to a Heh character\n",
        "    in a Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'p', u'h')\n",
        "def normalize_teh_marbuta_safebw(s):\n",
        "    \"\"\"Normalize all occurences of Teh Marbuta characters to a Heh character\n",
        "    in a Safe Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'p', u'h')\n",
        "def normalize_teh_marbuta_xmlbw(s):\n",
        "    \"\"\"Normalize all occurences of Teh Marbuta characters to a Heh character\n",
        "    in a XML Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'p', u'h')\n",
        "def normalize_teh_marbuta_hsb(s):\n",
        "    \"\"\"Normalize all occurences of Teh Marbuta characters to a Heh character\n",
        "    in a Habash-Soudi-Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'\\u0127', u'h')\n",
        "def normalize_teh_marbuta_ar(s):\n",
        "    \"\"\"Normalize all occurences of Teh Marbuta characters to a Heh character\n",
        "    in an Arabic string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return s.replace(u'\\u0629', u'\\u0647')\n",
        "def normalize_alef_bw(s):\n",
        "    \"\"\"Normalize various Alef variations to plain a Alef character in a\n",
        "    Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return _ALEF_NORMALIZE_BW_RE.sub(u'A', s)\n",
        "def normalize_alef_safebw(s):\n",
        "    \"\"\"Normalize various Alef variations to plain a Alef character in a\n",
        "    Safe Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return _ALEF_NORMALIZE_SAFEBW_RE.sub(u'A', s)\n",
        "def normalize_alef_xmlbw(s):\n",
        "    \"\"\"Normalize various Alef variations to plain a Alef character in a\n",
        "    XML Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return _ALEF_NORMALIZE_XMLBW_RE.sub(u'A', s)\n",
        "def normalize_alef_hsb(s):\n",
        "    \"\"\"Normalize various Alef variations to plain a Alef character in a\n",
        "    Habash-Soudi-Buckwalter encoded string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return _ALEF_NORMALIZE_HSB_RE.sub(u'A', s)\n",
        "def normalize_alef_ar(s):\n",
        "    \"\"\"Normalize various Alef variations to plain a Alef character in an\n",
        "    Arabic string.\n",
        "\n",
        "    Args:\n",
        "        s (:obj:`str`): The string to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        :obj:`str`: The normalized string.\n",
        "    \"\"\"\n",
        "\n",
        "    return _ALEF_NORMALIZE_AR_RE.sub(u'\\u0627', s)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caYn_710Qdh9"
      },
      "source": [
        "# code start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3INndgs2Phqw"
      },
      "source": [
        "\n",
        "def general_normalizer_1(sin):\n",
        "   s = normalize_unicode(sin)\n",
        "   s = normalize_alef_maksura_bw(s[:])\n",
        "   s = normalize_alef_maksura_safebw(s[:])\n",
        "   s = normalize_alef_maksura_xmlbw(s[:])\n",
        "   s = normalize_alef_maksura_hsb(s[:])\n",
        "   s = normalize_alef_maksura_ar(s[:])\n",
        "   s = normalize_teh_marbuta_bw(s[:])\n",
        "   s = normalize_teh_marbuta_safebw(s[:])\n",
        "   s = normalize_teh_marbuta_xmlbw(s[:])\n",
        "   s = normalize_teh_marbuta_hsb(s[:])\n",
        "   s = normalize_teh_marbuta_ar(s[:])\n",
        "   s = normalize_alef_bw(s[:])\n",
        "   s = normalize_alef_safebw(s[:])\n",
        "   s = normalize_alef_xmlbw(s[:])\n",
        "   s = normalize_alef_hsb(s[:])\n",
        "   s = normalize_alef_ar(s[:])\n",
        "   return s\n",
        "\n",
        "def general_normalizer_2(sin):\n",
        "    sout = araby.strip_diacritics(sin)\n",
        "\n",
        "    sout = sout.replace('\\f', ' ')\n",
        "    sout = sout.replace('\\r', ' ')\n",
        "    sout = sout.replace('(', ' ')\n",
        "    sout = sout.replace(')', ' ')\n",
        "    sout = sout.replace('(', ' ')\n",
        "    sout = sout.replace(')', ' ')\n",
        "    sout = sout.replace('[', ' ')\n",
        "    sout = sout.replace(']', ' ')\n",
        "    sout = sout.replace('}', ' ')\n",
        "    sout = sout.replace('{', ' ')\n",
        "\n",
        "    sout = sout.replace('‌', \"\")  # نیم فاصله\n",
        "    sout = sout.replace(\"ـ\", \"\")  # حروف کشیده\n",
        "    # print(sout)\n",
        "    sout = sout.replace('ء', '')\n",
        "    sout = sout.replace('ؤ', 'و')\n",
        "    sout = sout.replace('ئ', 'ی')\n",
        "    sout = sout.replace('ی', 'ی')\n",
        "    # print(sout)\n",
        "    sout = sout.replace('۩', '')\n",
        "    sout = sout.replace('ك', 'ک')\n",
        "    # print(sout)\n",
        "    sout = sout.replace('ٰ', 'ا')\n",
        "    sout = sout.replace('\\u200c', '')\n",
        "\n",
        "    sout = ' '.join(sout.split())  # remove multiple spaces\n",
        "    # print(sout , \"FINISH\")\n",
        "    return sout\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xKa2M61n91y"
      },
      "source": [
        "bad_alphabets = [  'پ' ,  'ژ' , 'چ' , 'گ']\n",
        "verbs_needs_alef = ['ملاقو', 'تتلو', 'یتلو', 'یدعو', 'یعفو', 'واولو', 'اولو', 'امرو', 'ویعفو','تبو','اندعو',  'باسطو',  'تبلو', 'اشکو', 'ادعو','لتتلو','یمحو','ندعو','ساتلو', 'یرجو', 'وادعو', 'اتلو','نتلو', 'لتنو','ترجو','مهلکو', 'لیربو','یربو','لتارکو','لذایقو', 'صالو','ویرجو','کاشفو','لیبلو','ونبلو','ونبلو','مرسلو','تدعو','لصالو']\n",
        "verbs_needs_alef_patt = \"(\"\n",
        "for el in verbs_needs_alef:\n",
        "    # if el == 'اولو' or el == 'واولو' :\n",
        "    #     el = 'واولو(?: الالباب)'\n",
        "    verbs_needs_alef_patt += F\"\\\\b{el}\\\\b|\"\n",
        "verbs_needs_alef_patt = verbs_needs_alef_patt[:-1]+\")\"\n",
        "num_of_verses = 6236\n",
        "num_of_suras = 114"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoyHHnsRPu4O"
      },
      "source": [
        "def input_normalizer(input):\n",
        "  for i in \"?!.؟!.,،()[]\":\n",
        "    input = input.replace(i , \" \")\n",
        "  normalized_in_1 = general_normalizer_2(input)\n",
        "  # print(\"1111  \",normalized_in_1)\n",
        "  normalized_in_2 = general_normalizer_1(normalized_in_1)\n",
        "  # print(\"2222  \",normalized_in_2)\n",
        "  return normalized_in_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBb6q7IEPzT8"
      },
      "source": [
        "def rule_maker(verse , qbigram_text, index):\n",
        "    #indexes = []\n",
        "    #[word1 , word2] = biword.split(\" \")\n",
        "    sentencelist = verse.split(\" \")\n",
        "    # for i in range(len(sentencelist)-1):\n",
        "    #   if re.match(biword_pattern, sentencelist[i:i+2]):\n",
        "    #       indexes.append(i)\n",
        "    # if sentencelist[i] == word1 and sentencelist[i+1] == word2:\n",
        "    #   indexes.append(i)\n",
        "    #rules = []\n",
        "    #for index in indexes:\n",
        "    rule = \"(?:\\\\b\" + qbigram_text + \"\\\\b)\"\n",
        "    for j in range(index-1,-1,-1):\n",
        "        # rule = \"(( \" + sentencelist[j] + \")?\"  + rule + \")\"\n",
        "        rule = \"(?:(?:\\\\b\" + sentencelist[j] + \"\\\\b )?\" + rule + \")\"\n",
        "    for j in range(index+2,len(sentencelist)):\n",
        "        rule = \"(?:(?:\" + rule + \")\" +  \"(?: \\\\b\" + sentencelist[j] + \"\\\\b)?)\"\n",
        "    # if index+2 < len(sentencelist):\n",
        "      # j=len(sentencelist)-1\n",
        "      # rule = \"((\"  + rule + \")\" +  \"(\" + sentencelist[j] + \")?)\"\n",
        "    #rules.append(rule)\n",
        "    return rule\n",
        "    #return rules\n",
        "def regexitize_quran(qbigram_bag):\n",
        "    \"Add regex patterns to quran\"\n",
        "\n",
        "    # va_pattern = \"((?:^| )\"+\"و\"+\" )\"\n",
        "    # va_repl = \"\\\\1?\"\n",
        "\n",
        "    #oo_pattern = \"([^ ]*\" + \"[^ (?: \" + \"ذ\" + \")( \" + \"بن\" + \")])\" + \"و\" + \"( |$)\"\n",
        "    #oo_repl = \"\\\\1\" + \"وا?\" + \"\\\\2\"\n",
        "\n",
        "    oo_pattern = F\"{verbs_needs_alef_patt}\"\n",
        "    oo_repl = \"\\\\1\"+\"ا\"+\"?\"\n",
        "\n",
        "    qdictionary_keys = list(qdictionary.keys())\n",
        "    for key in qdictionary_keys:\n",
        "\n",
        "        \"وا\"\n",
        "        new_verse = re.sub(oo_pattern, oo_repl, qdictionary[key])\n",
        "\n",
        "        \"و\"\n",
        "        #new_verse = re.sub(va_pattern, va_repl, new_verse)\n",
        "\n",
        "        \"الف کوچک\"\n",
        "        ######### new_key = f(new_key) ################\n",
        "        # it managed but not tested yet \n",
        "        # in normalizer\n",
        "\n",
        "        \"ک عربی\"\n",
        "        ######### new_key = f(new_key) ################\n",
        "        # it managed but not tested yet\n",
        "        # in normalizer\n",
        "\n",
        "        qdictionary[key] = new_verse\n",
        "\n",
        "    #repl = F\"{oo_pattern}\" + \"اْ\" + \"?\"\n",
        "    # for key in qdictionary.keys():\n",
        "    #     qdictionary[key] = re.sub(oo_pattern, repl, qdictionary[key])\n",
        "\n",
        "    # v_pattern = \"[ ^]*\" + \"و\" + \"[ |$]\"\n",
        "    # repl = F\"{v_pattern}\"+\"اْ\"+\"?\"\n",
        "    # for key in qdictionary.keys():\n",
        "    #     qdictionary[key] = re.sub(v_pattern, repl, qdictionary[key])\n",
        "    #[(re.findall(\".*[^ ]و\"+\"[ |$]\", qdictionary[ki]), ki) for ki in list(qdictionary.keys()) if len(re.findall(\".*[^ ]و\"+\"[ |$]\", qdictionary[ki]))!=0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yuTmpKaP_GV"
      },
      "source": [
        "Alireza_localrunning = False\n",
        "\"Create quranic dictionary\"\n",
        "if Alireza_localrunning:  \n",
        "  qtext = codecs.open(\"../output/v_seperated.txt\", 'r', 'utf-8').read()\n",
        "  qtext = qtext.split(\"\\n\")\n",
        "else:\n",
        "  url = \"https://raw.githubusercontent.com/aSafarpoor/open_repo_storhouse_for_nlp_Quran/main/v_seperated.txt\"\n",
        "  response = urllib.request.urlopen(url)\n",
        "  data = response.read()      # a `bytes` object\n",
        "  text = data.decode('utf-8')\n",
        "  qtext = text.split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdJ6TAqr8Fq"
      },
      "source": [
        "#next quran\n",
        "url = 'https://raw.githubusercontent.com/language-ml/nlp-exploring-datasets/main/religious_text/id_text_with_orthographies.txt'\n",
        "response = urllib.request.urlopen(url)\n",
        "data = response.read()      # a `bytes` object\n",
        "text = data.decode('utf-8')\n",
        "qtext2 = text.split(\"\\n\")\n",
        "qtext2 = qtext2[:-1] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhAbqX_Wpdfd"
      },
      "source": [
        "# Start phase 0 :) # for ali  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YnIizW2FScgk",
        "outputId": "0be386be-d24c-4a54-c37e-74e8ee354a35"
      },
      "source": [
        "qdictionary = {}\n",
        "num_of_verses = len(qtext)\n",
        "for i in qtext:\n",
        "  parted_verse = i.split(\"\\t\")\n",
        "  sura_verse_num = parted_verse[0]\n",
        "  verse = parted_verse[1]\n",
        "  qdictionary[sura_verse_num] = verse[:]\n",
        "qdictionary_keys = list((qdictionary.keys()))\n",
        "\n",
        "\"Normalize quran\"\n",
        "for i in range(num_of_verses):\n",
        "  sin = qdictionary[qdictionary_keys[i]]\n",
        "  sout1 = general_normalizer_1(sin)\n",
        "  sout2 = general_normalizer_2(sout1)\n",
        "  qdictionary[qdictionary_keys[i]] = sout2\n",
        "\n",
        "\n",
        "\"Save Normalized output\"\n",
        "# # file = codecs.open(\"normalized.txt\", \"w\", \"utf-8\")\n",
        "# # for i in range(6236):\n",
        "# #   file.write(qdictionary_keys[i])\n",
        "# #   file.write(\"\\t\")\n",
        "# #   file.write(qdictionary[qdictionary_keys[i]])\n",
        "# #   file.write(\"\\n\")\n",
        "# # file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Save Normalized output'"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "kfpa3pV7rz1o",
        "outputId": "24089419-f11b-474c-9a02-032427ca53ab"
      },
      "source": [
        "#next quran:\n",
        "qdictionary2 = {}\n",
        "num_of_verses2 = len(qtext2)\n",
        "\n",
        "for i in qtext:\n",
        "  parted_verse2 = i.split(\"\\t\")\n",
        "  sura_verse_num2 = parted_verse2[0]\n",
        "  verse2 = parted_verse2[1]\n",
        "  qdictionary2[sura_verse_num2] = verse2[:]\n",
        "qdictionary_keys2 = list((qdictionary2.keys()))\n",
        "\n",
        "\"Normalize quran\"\n",
        "print(num_of_verses2)\n",
        "print(len(qdictionary_keys2))\n",
        "for i in range(num_of_verses2):\n",
        "  sin = qdictionary2[qdictionary_keys2[i]]\n",
        "  sout1 = general_normalizer_1(sin)\n",
        "  sout2 = general_normalizer_2(sout1)\n",
        "  qdictionary[qdictionary_keys2[i]] = sout2\n",
        "\n",
        "\n",
        "\"Save Normalized output\"\n",
        "# # file = codecs.open(\"normalized.txt\", \"w\", \"utf-8\")\n",
        "# # for i in range(6236):\n",
        "# #   file.write(qdictionary_keys[i])\n",
        "# #   file.write(\"\\t\")\n",
        "# #   file.write(qdictionary[qdictionary_keys[i]])\n",
        "# #   file.write(\"\\n\")\n",
        "# # file.close()\n",
        "\n",
        "\"Add regex patterns to quran\"\n",
        "regexitize_quran(qdictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6236\n",
            "6236\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Save Normalized output'"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UYnPEH9rpvaN",
        "outputId": "02ce1bb7-cbc7-4b38-d60f-9d6635cfb6a5"
      },
      "source": [
        "qdictionary[qdictionary_keys[0]] \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'بسم الله الرحمن الرحيم'"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVe4pfIRP6S9"
      },
      "source": [
        "# input = \"   فِی قَوْلِهِ تَعَالَی قُلْ هذِهِ سَبِیلِی أَدْعُوا إِلَی اللهِ عَلی بَصِیرَةٍ أَنَا وَ مَنِ اتَّبَعَنِی قَالَ هِیَ وَلَایَتُنَا أَهْلَ الْبَیْتِ\"\n",
        "# input = \" « آ» به شکل ( ٰ ) نوشته می‌شود +++ تبدیل حرفی(ک) به حرفی دیگر(ك)\"\n",
        "# input = \"نحوه فارسی(کشــــــــــــیده‌نویسی)\"\n",
        "# input = \" قـــــــــــــــــــــــــــــــــــ          ــل هو اللــــه احد تایپ حروف کشــــــــیده شده\"\n",
        "#input = \"حضرت موسیٰ (علیٰ نبیّنا و آله و علیه السّلام) به خداوند متعال عرض کرد: رَبَّنا اِنَّکَ ءاتَیتَ فِرعَونَ وَ مَلَاَهُ زینَةً\"\n",
        "#input = \"خداوند متعال در جواب فرمود: قالَ قَد اُجیبَت دَعوَتُکُما؛دعای شما دو نفر را -موسیٰ و هارون را- ما قبول کردیم، مستجاب کردیم، امّا شرط دارد: فَاستَقیما وَ لا تَتَّبِعآنِّ سَبیلَ الَّذینَ لا یَعلَمون؛(۲) بِایستید، استقامت کنید. استقامت در میدان جنگ نظامی یک‌جور استً\"\n",
        "#input = \"آیه را زیر لب زمزمه می‌کرد که أَمْ حَسِبْتَ أَنَّ أَصْحَبَ ٱلْكَهْفِ وَٱلرَّقِيمِ كَانُواْ مِنْ ءَايَتِنَا عَجَبًا، آیا اصحاب کهف را از عجایب آیات ما می‌پنداری؟ِ\"\n",
        "#input = \"رب العالمین همه چیزی خیلی خوب پیش رفت.\"\n",
        "\n",
        "# test1 = 'قالوا ربنا' #passed\n",
        "test2 = 'وَهُوَ مِنَ الصَّادِقِينَ' #--> va problem\n",
        "# test3 = 'خالدین فیها' # not passed\n",
        "# test4 = 'ان الله غفور 4##96' # pass\n",
        "# test5 = 'والعصر 103##1' # val problem\n",
        "# test6 = 'الله دز آسمان‌ها فقط نیست. در همه جاست.' #passed\n",
        "# test7 = 'حضرت عیس مسیح پیش از پیامبر خاتم به پیامبری رسید'  # passed\n",
        "# test8 = 'خداوند در دل های شکسته قرار دارد' # passed\n",
        "# test9 = 'بسیاری از شاعران ما درباره‌ی طور سینین شعر گفته‌اند.' # so bad\n",
        "# test10 = 'پروردگارا ما را به صراط مستقيم هدایت کن' # a little problem\n",
        "# test11 = 'تو اول بسمِ الله را بگو و بعد کار را شروع کن' #problem\n",
        "# test12 = 'قوم إبراهيم وقوم لوط'\n",
        "# test13 = 'خداوند چگونه در قرآن انسان را مورد خطاب قرار می‌دهد؟' # still problem\n",
        "# test14 = 'اگر بدانیم مقصود خداوند از أُولَٰئِكَ هُمُ الْوَارِثُونَ همان ما انسان‌های وارسته هستیم، دیگر بسیاری از رفتارها را تکرار نمی‌کنیم'\n",
        "# test16 = 'وَالَّذِينَ هُمْ عَلَىٰ صَلَوَاتِهِمْ يُحَافِظُونَ'\n",
        "# test17 = 'سرزمین مصر، سرزمین فراعنه'\n",
        "# test18 = 'و آنجا که حضرت یوسف می‌فرماید: مَعَاذَ اللَّهِ ۖ إِنَّهُ رَبِّي أَحْسَنَ مَثْوَايَ ۖ إِنَّهُ لَا يُفْلِحُ الظَّالِمُونَ'\n",
        "test19 = 'آیا آیه‌ی إِنَّمَا وَلِيُّكُمُ اللَّهُ وَرَسُولُهُ وَالَّذِينَ آمَنُوا الَّذِينَ يُقِيمُونَ الصَّلَاةَ وَيُؤْتُونَ الزَّكَاةَ وَهُمْ رَاكِعُونَ دربارهی            حضرت         علی (علیه‌السلام) است؟' #problem\n",
        "# test20 = 'شان نزول آیه ی انما وليكم اللّه ورسوله والّذين امنوا' # problem\n",
        "#input = 'سلاممممم           ممممممم     چطوری ssss'\n",
        "input = test2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0hOeI2EPmBO"
      },
      "source": [
        "\"Creating token bag\"\n",
        "qbigram_bag = {}\n",
        "qdictionary_keys = list(qdictionary.keys())\n",
        "for i in range(len(qdictionary)):\n",
        "  k_name = qdictionary_keys[i]\n",
        "  verse = qdictionary[k_name]\n",
        "  temp = verse.split(\" \")\n",
        "  for j in range(len(temp)-1):\n",
        "      try:\n",
        "        qbigram_bag[temp[j] + \" \" + temp[j+1]].append((k_name,j))\n",
        "      except:\n",
        "        qbigram_bag[temp[j] + \" \" + temp[j+1]] = [(k_name,j)]\n",
        "      if temp[j] == 'و' :\n",
        "        try:\n",
        "          qbigram_bag[temp[j]  + temp[j+1]].append((k_name,j))\n",
        "        except:\n",
        "          qbigram_bag[temp[j]  + temp[j+1]] = [(k_name,j)]\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIAXjsZNqtmj"
      },
      "source": [
        "# c = 0 \n",
        "# for i in list(qbigram_bag.keys())[:2]:\n",
        "#   print(c)\n",
        "#   c+=1\n",
        "#   print(i)\n",
        "#   print(qbigram_bag[i])\n",
        "#   print(' ')\n",
        "\n",
        "# print(len(qbigram_bag.keys())) \n",
        "# ##### older format was : 41978\n",
        "# ##### newer format is  : 44414\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBdpXC2qjSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a683495-ca3e-4668-ee1b-53d210c3532f"
      },
      "source": [
        "\n",
        "#rare_input = input[:]\n",
        "input_normd = input_normalizer(input)\n",
        "print(input_normd)\n",
        "input_list = input_normd.split(\" \")\n",
        "input_bigram = []\n",
        "for i in range(len(input_list)-1):\n",
        "  # flag = True\n",
        "  # for alphabet in bad_alphabets:\n",
        "  #   if alphabet in input_list[i] or alphabet in input_list[i+1]:\n",
        "  #     flag = False\n",
        "  # if flag:\n",
        "  #   input_bigram.append(input_list[i] + \" \" + input_list[i+1])\n",
        "  if not any(alphabet in input_list[i] or alphabet in input_list[i+1] for alphabet in bad_alphabets):\n",
        "      input_bigram.append(input_list[i] + \" \" + input_list[i+1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"re.find on all the quranic regexitized bigram and input bigram\"\n",
        "qbigram_bag_keys = list(qbigram_bag.keys())\n",
        "qbigram_found_list = []\n",
        "# for bigram in input_bigram:\n",
        "#   if bigram in qbigram_bag_keys:\n",
        "#     for i in qbigram_bag[bigram]:\n",
        "#       search_found_list.append([i,bigram])\n",
        "#qbigram_found_list = [(qbigram, qbigram_bag[qbigram]) for qbigram in qbigram_bag_keys if len(re.findall(qbigram, input_normd))!=0]\n",
        "qbigram_found_list = []\n",
        "for qbigram in qbigram_bag_keys:\n",
        "    pattern = qbigram\n",
        "    pattern.replace(\" \", \"\\\\b \\\\b\")\n",
        "    pattern = \"\\\\b\"+pattern+\"\\\\b\"\n",
        "    if len(re.findall(pattern, input_normd))!=0:\n",
        "        qbigram_found_list.append((qbigram, qbigram_bag[qbigram]))\n",
        "        #qbigram_found_list.extend([[id, qbigram] for id in qbigram_bag[qbigram]])\n",
        "\n",
        "\n",
        "    # matches = list(re.findall(qbigram, input_normd))\n",
        "    # if len(matches) != 0:\n",
        "    #     for match in matches:\n",
        "    #         for id in qbigram_bag[qbigram]:\n",
        "    #             verse = qdictionary[id]\n",
        "    #             rules = rule_maker(verse, qbigram, start, end)\n",
        "\n",
        "output_bag = []\n",
        "for qbigram in qbigram_found_list:\n",
        "    for inner_tup in qbigram[1]:\n",
        "        id, index = inner_tup\n",
        "        qbigram_text = qbigram[0]\n",
        "        verse = qdictionary[id]\n",
        "        #rules = rule_maker(verse ,qbigram[0], tup[1])\n",
        "        rule = rule_maker(verse ,qbigram_text, index)\n",
        "        #for rule in rules:\n",
        "        \n",
        "        matches = list(re.finditer(rule,input_normd))\n",
        "        # if isinstance(matches, list):\n",
        "        #     temp = \"\"\n",
        "        #     for match in matches:\n",
        "        #         if(len(match) > len(temp)):\n",
        "        #             temp = sample[:]\n",
        "        #         output_bag.append([tup[0],temp])\n",
        "        # elif isinstance(subout, str):\n",
        "        if len(matches)!=0:\n",
        "            for match in matches:\n",
        "              if F\"{input_normd[match.regs[0][0]:match.regs[0][1]]} {id}\" not in output_bag:\n",
        "                output_bag.append(F\"{input_normd[match.regs[0][0]:match.regs[0][1]]} {id}\")\n",
        "\n",
        "# print(len(output_bag), ' 666666666666')\n",
        "\n",
        "for qbigram in qbigram_found_list:\n",
        "    for inner_tup in qbigram[1]:\n",
        "        id, index = inner_tup\n",
        "        qbigram_text = qbigram[0]\n",
        "        verse = qdictionary2[id]\n",
        "        #rules = rule_maker(verse ,qbigram[0], tup[1])\n",
        "        rule = rule_maker(verse ,qbigram_text, index)\n",
        "        #for rule in rules:\n",
        "        \n",
        "        matches = list(re.finditer(rule,input_normd))\n",
        "        # if isinstance(matches, list):\n",
        "        #     temp = \"\"\n",
        "        #     for match in matches:\n",
        "        #         if(len(match) > len(temp)):\n",
        "        #             temp = sample[:]\n",
        "        #         output_bag.append([tup[0],temp])\n",
        "        # elif isinstance(subout, str):\n",
        "        if len(matches)!=0:\n",
        "            for match in matches:\n",
        "              if F\"{input_normd[match.regs[0][0]:match.regs[0][1]]} {id}\" not in output_bag:\n",
        "                output_bag.append(F\"{input_normd[match.regs[0][0]:match.regs[0][1]]} {id}\")\n",
        "\n",
        "# print(len(output_bag) , \" 99999999\")\n",
        "# for i in output_bag:\n",
        "  # print(i)\n",
        "# print(output_bag)\n",
        "\n",
        "# output_bag.sort()\n",
        "# new_list = [output_bag[0]]\n",
        "# for i in range(1,len(output_bag)):\n",
        "#   if output_bag[i] == new_list[-1] :\n",
        "#     pass\n",
        "#   elif output_bag[i][0] == new_list[-1][0]:\n",
        "#     if new_list[-1][0] in output_bag[i][1]:\n",
        "#       new_list[-1] = output_bag[i]\n",
        "#   else:\n",
        "#       new_list.append(output_bag[i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "وهو من الصادقين\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_cEGuVDyJMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc69014-b7d2-434d-8305-7296ce789864"
      },
      "source": [
        "len(qbigram_found_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUCCICRLlRYJ"
      },
      "source": [
        "# print(len(input))\n",
        "# print(len(input_normd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJp9XN33eHKQ"
      },
      "source": [
        "# Mehdii editted code with ALi;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSZg4S4beGnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d888816-4987-4161-a279-19362e7e17ef"
      },
      "source": [
        "def print_input(out_list , input_org, input_norm):\n",
        "  input_org_list = input_org.split(\" \")\n",
        "  input_norm_list = input_norm.split(\" \")\n",
        "  # print(list(enumerate(input_org_list)))\n",
        "  # print(list(enumerate(input_norm_list)))\n",
        "  \n",
        "  final_out_list = []\n",
        "\n",
        "  for text in out_list:\n",
        "    try:\n",
        "      text_list = text.split(\" \")\n",
        "      quran_num = text_list[-1] \n",
        "      result = \" \"\n",
        "      for i in text_list[0:-1]:\n",
        "        try:\n",
        "          # print(input_normalizer(str(i)) , input_norm_list[1] , input_normalizer(str(i))== input_norm_list[i] )\n",
        "          try:\n",
        "            item_index = input_norm_list.index(input_normalizer(str(i)))\n",
        "            result += input_org_list[int(item_index)]+\" \"\n",
        "            # print(item_index , result )\n",
        "          except:\n",
        "            pass\n",
        "        except Exception as e:\n",
        "          continue\n",
        "      result += str(quran_num)\n",
        "      temp_list = list(filter(lambda b: b != '', result.split(\" \")))\n",
        "      # print(temp_list)\n",
        "      if len(temp_list) >2:\n",
        "\n",
        "        if result not in final_out_list:\n",
        "          final_out_list.append(result)\n",
        "    except Exception as x:\n",
        "      continue\n",
        "      \n",
        "  return final_out_list\n",
        "\n",
        "x = print_input(out_list = output_bag , input_org = input , input_norm = input_normd )\n",
        "\n",
        "print(input)\n",
        "print(input_normd)\n",
        "\n",
        "\n",
        "flag = True\n",
        "for i in x:\n",
        "  temp = i.split(\" \")\n",
        "  temp = list(filter(lambda a: a != ' ', temp))\n",
        "  if len(temp) > 4:\n",
        "    print(i)\n",
        "    flag = False\n",
        "\n",
        "if flag or len(x) == 0:\n",
        "  print(\"nothing found\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "وَهُوَ مِنَ الصَّادِقِينَ\n",
            "وهو من الصادقين\n",
            " وَهُوَ مِنَ الصَّادِقِينَ 12##27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYwLfSy9EM1k"
      },
      "source": [
        "# Mehdii code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIHqeYlQSz05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3819537a-d5e8-439b-ad7a-5e473b4b6833"
      },
      "source": [
        "def print_input(out_list , input_org, input_norm):\n",
        "  input_org_list = input_org.split(\" \")\n",
        "  input_norm_list = input_norm.split(\" \")\n",
        "  final_out_list = []\n",
        "\n",
        "  for text in out_list:\n",
        "    try:\n",
        "      text_list = text.split(\" \")\n",
        "      quran_num = text_list[-1] \n",
        "      result = \" \"\n",
        "      for i in text_list[0:-1]:\n",
        "        try:\n",
        "          item_index = input_norm_list.index(str(i))\n",
        "          result += input_org_list[int(item_index)]+\" \"\n",
        "        except Exception as e:\n",
        "          continue\n",
        "      result += str(quran_num)\n",
        "      if len(list(result.split(\" \"))) >3 :\n",
        "        final_out_list.append(result)\n",
        "    except Exception as x:\n",
        "      continue\n",
        "      \n",
        "  return final_out_list\n",
        "\n",
        "x = print_input(out_list = output_bag , input_org = input , input_norm = input_normd )\n",
        "\n",
        "print(input)\n",
        "for i in x:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "وَهُوَ مِنَ الصَّادِقِينَ\n",
            " وَهُوَ مِنَ 2##85\n",
            " وَهُوَ مِنَ 2##91\n",
            " وَهُوَ مِنَ 3##39\n",
            " وَهُوَ مِنَ 3##85\n",
            " وَهُوَ مِنَ 4##92\n",
            " وَهُوَ مِنَ 4##108\n",
            " وَهُوَ مِنَ 5##5\n",
            " وَهُوَ مِنَ 6##14\n",
            " وَهُوَ مِنَ 6##98\n",
            " وَهُوَ مِنَ 6##99\n",
            " وَهُوَ مِنَ 6##114\n",
            " وَهُوَ مِنَ 6##141\n",
            " وَهُوَ مِنَ 7##57\n",
            " وَهُوَ مِنَ 11##7\n",
            " وَهُوَ مِنَ 12##26\n",
            " وَهُوَ مِنَ الصَّادِقِينَ 12##27\n",
            " وَهُوَ مِنَ 13##3\n",
            " وَهُوَ مِنَ 16##14\n",
            " وَهُوَ مِنَ 16##76\n",
            " وَهُوَ مِنَ 18##37\n",
            " وَهُوَ مِنَ 25##48\n",
            " وَهُوَ مِنَ 25##54\n",
            " وَهُوَ مِنَ 41##44\n",
            " وَهُوَ مِنَ 42##28\n",
            " وَهُوَ مِنَ 47##2\n",
            " وَهُوَ مِنَ 48##24\n",
            " مِنَ الصَّادِقِينَ 12##27\n",
            " مِنَ الصَّادِقِينَ 7##70\n",
            " مِنَ الصَّادِقِينَ 7##106\n",
            " مِنَ الصَّادِقِينَ 11##32\n",
            " مِنَ الصَّادِقِينَ 12##27\n",
            " مِنَ الصَّادِقِينَ 15##7\n",
            " مِنَ الصَّادِقِينَ 24##9\n",
            " مِنَ الصَّادِقِينَ 26##31\n",
            " مِنَ الصَّادِقِينَ 26##154\n",
            " مِنَ الصَّادِقِينَ 26##187\n",
            " مِنَ الصَّادِقِينَ 29##29\n",
            " مِنَ الصَّادِقِينَ 46##22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrL5Xc3bERWx"
      },
      "source": [
        "# qbigram_bag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHUhwtZo17a"
      },
      "source": [
        "org_input = 'وَهُوَ مِنَ الصَّادِقِينَ'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCcQ24XMEBYG"
      },
      "source": [
        "input_norm = \"وهو من الصادقين\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu6bWnpCEO4s"
      },
      "source": [
        "# test_list = output_bag[-5:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYUJDS6RE99g"
      },
      "source": [
        "test_list = ['من الصادقين 6###5']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PPazpoJENgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eff8c8b-7330-4913-fd3d-3f6ba85da7ab"
      },
      "source": [
        "xx =print_input(out_list = output_bag ,input_org = org_input , input_norm = input_norm)\n",
        "print(xx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' وَهُوَ مِنَ 2##85', ' وَهُوَ مِنَ 2##91', ' وَهُوَ مِنَ 3##39', ' وَهُوَ مِنَ 3##85', ' وَهُوَ مِنَ 4##92', ' وَهُوَ مِنَ 4##108', ' وَهُوَ مِنَ 5##5', ' وَهُوَ مِنَ 6##14', ' وَهُوَ مِنَ 6##98', ' وَهُوَ مِنَ 6##99', ' وَهُوَ مِنَ 6##114', ' وَهُوَ مِنَ 6##141', ' وَهُوَ مِنَ 7##57', ' وَهُوَ مِنَ 11##7', ' وَهُوَ مِنَ 12##26', ' وَهُوَ مِنَ الصَّادِقِينَ 12##27', ' وَهُوَ مِنَ 13##3', ' وَهُوَ مِنَ 16##14', ' وَهُوَ مِنَ 16##76', ' وَهُوَ مِنَ 18##37', ' وَهُوَ مِنَ 25##48', ' وَهُوَ مِنَ 25##54', ' وَهُوَ مِنَ 41##44', ' وَهُوَ مِنَ 42##28', ' وَهُوَ مِنَ 47##2', ' وَهُوَ مِنَ 48##24', ' مِنَ الصَّادِقِينَ 12##27', ' مِنَ الصَّادِقِينَ 7##70', ' مِنَ الصَّادِقِينَ 7##106', ' مِنَ الصَّادِقِينَ 11##32', ' مِنَ الصَّادِقِينَ 12##27', ' مِنَ الصَّادِقِينَ 15##7', ' مِنَ الصَّادِقِينَ 24##9', ' مِنَ الصَّادِقِينَ 26##31', ' مِنَ الصَّادِقِينَ 26##154', ' مِنَ الصَّادِقِينَ 26##187', ' مِنَ الصَّادِقِينَ 29##29', ' مِنَ الصَّادِقِينَ 46##22']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQY2YHDMG5C-",
        "outputId": "eb82c90f-8adb-4eba-a877-43ec7369faaf"
      },
      "source": [
        "xx =print_input(out_list = test_list,input_org = org_input , input_norm = input_norm)\n",
        "print(xx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' مِنَ الصَّادِقِينَ 6###5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EazRoXIhJ6x8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}