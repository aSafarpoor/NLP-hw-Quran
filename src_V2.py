# -*- coding: utf-8 -*-
"""Quran2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v3WOt_N7WKQ5gDPYJk3yNn3Zh8s6SXpt
"""

# Commented out IPython magic to ensure Python compatibility.
# % pip install pyarabic

# Commented out IPython magic to ensure Python compatibility.
# %pip install camel_tools

import re
import unicodedata
import requests
import codecs
import pyarabic.araby as araby
from camel_tools.utils.charmap import CharMapper
import urllib.request

"""# alireza code ... now commented"""

# import xml.etree.ElementTree as et
# import codecs

# xtree = et.parse(R"Quran Data/Quran_Words.xml")
# xroot = xtree.getroot()
# quran = []
# i = 0
# while i<len(xroot):
#     sureh_num = xroot[i].attrib['sureh']
#     aye_num = xroot[i].attrib['aye']
#     if aye_num=="1" and sureh_num!="1" and sureh_num!="9":
#         i=i+1
#         continue

#     verse = []
#     while True:
#         if i<len(xroot) and xroot[i].attrib['aye'] == aye_num:
#             verse.append(xroot[i].attrib['entry'])
#             i = i+1
#         else:
#             break
#     sureh_num = int(sureh_num)
#     aye_num = int(aye_num)
#     if sureh_num != 1 and sureh_num != 9:
#         aye_num = aye_num - 1
#     id = F"{sureh_num}##{aye_num}"
#     quran.append(F"{id}\t{' '.join(verse)}")

# with codecs.open("output/v_seperated.txt", 'w', 'utf-8') as f:
#     for line in quran:
#         f.write(line+'\n')

"""reading Alireza output text from github and make it ready on Quran list:

# from internet, copied to change if necessary
"""

aa = "َ"
ee = "ِ"
oo = "ُ"
va = "و"
_ALEF_NORMALIZE_BW_RE = re.compile(u'[<>{|]')
_ALEF_NORMALIZE_SAFEBW_RE = re.compile(u'[IOLM]')
_ALEF_NORMALIZE_XMLBW_RE = re.compile(u'[IO{|]')
_ALEF_NORMALIZE_HSB_RE = re.compile(u'[\u0102\u00c2\u00c4\u0100]')
_ALEF_NORMALIZE_AR_RE = re.compile(u'[\u0625\u0623\u0671\u0622]')
_UNICODE_CHAR_FIX = CharMapper({
                        '\ufdfc': 'ريال',
                        '\ufdfd': 'بسم الله الرحمن الرحيم',
                    })
def normalize_unicode(s, compatibility=True):
    """Normalize Unicode strings into their canonically composed form or
    (i.e. characters that can be written as a combination of unicode characters
    are converted to their single character form).

    Note: This is essentially a call to :func:`unicodedata.normalize` with
    form 'NFC' if **compatibility** is False or 'NFKC' if it's True.

    Args:
        s (:obj:`str`): The string to be normalized.
        compatibility (:obj:`bool`, optional): Apply compatibility
            decomposition. Defaults to True.

    Returns:
        :obj:`str`: The normalized string.
    """

    if compatibility:
        fixed = _UNICODE_CHAR_FIX(s)
        return unicodedata.normalize('NFKC', fixed)

    return unicodedata.normalize('NFC', s)
def normalize_alef_maksura_bw(s):
    """Normalize all occurences of Alef Maksura characters to a Yeh character
    in a Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'Y', u'y')
def normalize_alef_maksura_safebw(s):
    """Normalize all occurences of Alef Maksura characters to a Yeh character
    in a Safe Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'Y', u'y')
def normalize_alef_maksura_xmlbw(s):
    """Normalize all occurences of Alef Maksura characters to a Yeh character
    in a XML Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'Y', u'y')
def normalize_alef_maksura_hsb(s):
    """Normalize all occurences of Alef Maksura characters to a Yeh character
    in a Habash-Soudi-Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'\u00fd', u'y')
def normalize_alef_maksura_ar(s):
    """Normalize all occurences of Alef Maksura characters to a Yeh character
    in an Arabic string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'\u0649', u'\u064a')
def normalize_teh_marbuta_bw(s):
    """Normalize all occurences of Teh Marbuta characters to a Heh character
    in a Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'p', u'h')
def normalize_teh_marbuta_safebw(s):
    """Normalize all occurences of Teh Marbuta characters to a Heh character
    in a Safe Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'p', u'h')
def normalize_teh_marbuta_xmlbw(s):
    """Normalize all occurences of Teh Marbuta characters to a Heh character
    in a XML Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'p', u'h')
def normalize_teh_marbuta_hsb(s):
    """Normalize all occurences of Teh Marbuta characters to a Heh character
    in a Habash-Soudi-Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'\u0127', u'h')
def normalize_teh_marbuta_ar(s):
    """Normalize all occurences of Teh Marbuta characters to a Heh character
    in an Arabic string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return s.replace(u'\u0629', u'\u0647')
def normalize_alef_bw(s):
    """Normalize various Alef variations to plain a Alef character in a
    Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return _ALEF_NORMALIZE_BW_RE.sub(u'A', s)
def normalize_alef_safebw(s):
    """Normalize various Alef variations to plain a Alef character in a
    Safe Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return _ALEF_NORMALIZE_SAFEBW_RE.sub(u'A', s)
def normalize_alef_xmlbw(s):
    """Normalize various Alef variations to plain a Alef character in a
    XML Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return _ALEF_NORMALIZE_XMLBW_RE.sub(u'A', s)
def normalize_alef_hsb(s):
    """Normalize various Alef variations to plain a Alef character in a
    Habash-Soudi-Buckwalter encoded string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return _ALEF_NORMALIZE_HSB_RE.sub(u'A', s)
def normalize_alef_ar(s):
    """Normalize various Alef variations to plain a Alef character in an
    Arabic string.

    Args:
        s (:obj:`str`): The string to be normalized.

    Returns:
        :obj:`str`: The normalized string.
    """

    return _ALEF_NORMALIZE_AR_RE.sub(u'\u0627', s)

"""# code start"""

def general_normalizer_1(sin):
   s = normalize_unicode(sin)
   s = normalize_alef_maksura_bw(s[:])
   s = normalize_alef_maksura_safebw(s[:])
   s = normalize_alef_maksura_xmlbw(s[:])
   s = normalize_alef_maksura_hsb(s[:])
   s = normalize_alef_maksura_ar(s[:])
   s = normalize_teh_marbuta_bw(s[:])
   s = normalize_teh_marbuta_safebw(s[:])
   s = normalize_teh_marbuta_xmlbw(s[:])
   s = normalize_teh_marbuta_hsb(s[:])
   s = normalize_teh_marbuta_ar(s[:])
   s = normalize_alef_bw(s[:])
   s = normalize_alef_safebw(s[:])
   s = normalize_alef_xmlbw(s[:])
   s = normalize_alef_hsb(s[:])
   s = normalize_alef_ar(s[:])
   return s
def general_normalizer_2(sin):
    sout = araby.strip_diacritics(sin)

    sout = sout.replace('\f', ' ')
    sout = sout.replace('\r', ' ')
    sout = sout.replace('(', ' ')
    sout = sout.replace(')', ' ')
    sout = sout.replace('(', ' ')
    sout = sout.replace(')', ' ')
    sout = sout.replace('[', ' ')
    sout = sout.replace(']', ' ')
    sout = sout.replace('}', ' ')
    sout = sout.replace('{', ' ')

    sout = sout.replace('‌', "")  # نیم فاصله
    sout = sout.replace("ـ", "")  # حروف کشیده
    # print(sout)
    sout = sout.replace('ء', '')
    sout = sout.replace('ؤ', 'و')
    sout = sout.replace('ئ', 'ی')
    sout = sout.replace('ی', 'ی')
    # print(sout)
    sout = sout.replace('۩', '')
    sout = sout.replace('ك', 'ک')
    # print(sout)
    sout = sout.replace('ٰ', 'ا')
    sout = sout.replace('\u200c', '')
    sout = sout.replace('ي', 'ی')

    sout = ' '.join(sout.split())  # remove multiple spaces
    # print(sout , "FINISH")
    return sout
def input_normalizer(input):
  for i in "?!.؟!.,،()[]":
    input = input.replace(i , " ")
  normalized_in_1 = general_normalizer_1(input)
  #print("1111  ",normalized_in_1)
  normalized_in_2 = general_normalizer_2(normalized_in_1)
  #print("2222  ",normalized_in_2)
  return normalized_in_2
def rule_maker(verse , qbigram_text, index):
    #indexes = []
    #[word1 , word2] = biword.split(" ")
    sentencelist = verse.split(" ")
    # for i in range(len(sentencelist)-1):
    #   if re.match(biword_pattern, sentencelist[i:i+2]):
    #       indexes.append(i)
    # if sentencelist[i] == word1 and sentencelist[i+1] == word2:
    #   indexes.append(i)
    #rules = []
    #for index in indexes:
    #qbigram_text = qbigram_text.replace(" ", " ?")
    #rule = "(?:\\b" + qbigram_text + "\\b)"
    rule = "(?:" + qbigram_text + ")"
    for j in range(0,index):
        # rule = "(( " + sentencelist[j] + ")?"  + rule + ")"
        #rule = "(?:(?:\\b" + " ".join(sentencelist[j:index]) + "\\b )?" + rule + ")"
        without_last_space = True if qbigram_text[:8] == F'(?:^| ){va}' else False
        left_regexd, neet_to_handle_va = regexitize_verse(" ".join(sentencelist[j:index]), without_last_space= without_last_space)
        before_bigram = F'(^|{va}|{va} | )' if neet_to_handle_va else ''
        rule = "(?:" + before_bigram + qbigram_text[2:] + ")" if neet_to_handle_va and j == 0 else rule
        rule = "(?:(?:" + left_regexd + ")?" +  rule + ")"
    for j in range(len(sentencelist), index+2, -1):
        #rule = "(?:(?:" + rule + ")" +  "(?: \\b" + " ".join(sentencelist[index+2:j+1]) + "\\b)?)"
        va_end_bigram = True if qbigram_text[-3:] == F'{va} ?' else False
        right_regexd, _ = regexitize_verse(" ".join(sentencelist[index+2:j]), va_before= va_end_bigram, without_last_space= True)
        after_bigram = '' if va_end_bigram else ' '
        rule = "(?:(?:" + rule + ")" + '(?:'+ after_bigram + right_regexd + ')?)'
    # if index+2 < len(sentencelist):
      # j=len(sentencelist)-1
      # rule = "(("  + rule + ")" +  "(" + sentencelist[j] + ")?)"
    #rules.append(rule)
    return rule
def regexitize_qdictionary(qdictionary):
    "Add regex patterns to quran verses"
    #
    # va_pattern = "((?:^| )"+"و"+" )"
    # va_repl = "\\1?"

    #oo_pattern = "([^ ]*" + "[^ (?: " + "ذ" + ")( " + "بن" + ")])" + "و" + "( |$)"
    #oo_repl = "\\1" + "وا?" + "\\2"

    oo_pattern = F"{verbs_needs_alef_patt}"
    oo_repl = "\\1"+"ا"+"?"

    qdictionary_keys = list(qdictionary.keys())
    for key in qdictionary_keys:

        "وا"
        new_verse = re.sub(oo_pattern, oo_repl, qdictionary[key])

        "و"
        #new_verse = re.sub(va_pattern, va_repl, new_verse)

        qdictionary[key] = new_verse
def regexitize_qbigrambag(qbigram_bag):
    "Add regex patterns to quran bigrams"

    va_pattern1 = "(^"+"و"+" )"
    va_repl1 = "(?:^| )\\1?"
    va_pattern2 = "( "+"و"+"$)"
    va_repl2 = "\\1 ?"

    qbigram_bag_keys = list(qbigram_bag.keys())
    for key in qbigram_bag_keys:
        new_key = key
        word1, word2 = new_key.split(" ")
        if 'و' == word1:
            new_key.replace(" ", " \\b")
            new_key = new_key + "\\b"
            new_key = re.sub(va_pattern1, va_repl1, new_key)
        elif 'و' == word2:
            new_key.replace(" ", "\\b ")
            new_key = "\\b" + new_key
            new_key = re.sub(va_pattern2, va_repl2, new_key)
        else:
            new_key.replace(" ", "\\b \\b")
            new_key = "\\b" + new_key + "\\b"

        # "و"
        # new_key = re.sub(va_pattern1, va_repl1, new_key)
        # new_key = re.sub(va_pattern2, va_repl2, new_key)

        qbigram_bag[new_key] = qbigram_bag.pop(key)
# def regexitize_qlist(qlist):
#     "Add regex patterns to list of quranic words"
#
#     va_pattern1 = "(^" + "و" + " )"
#     va_repl1 = "(?:^| )\\1?"
#     va_pattern2 = "( " + "و" + "$)"
#     va_repl2 = "\\1 ?"
#     va_pattern3 = "( " + "و" + " )"
#     va_repl3 = "\\1?"
#
#     for qword in qlist:
#         if 'و' == word1:
#             new_key.replace(" ", " \\b")
#             new_key = new_key + "\\b"
#             new_key = re.sub(va_pattern1, va_repl1, new_key)
#         elif 'و' == word2:
#             new_key.replace(" ", "\\b ")
#             new_key = "\\b" + new_key
#             new_key = re.sub(va_pattern2, va_repl2, new_key)
#         else:
#             new_key.replace(" ", "\\b \\b")
#             pattern = "\\b" + pattern + "\\b"
#
#         # "و"
#         # new_key = re.sub(va_pattern1, va_repl1, new_key)
#         # new_key = re.sub(va_pattern2, va_repl2, new_key)
#
#         qbigram_bag[new_key] = qbigram_bag.pop(key)
def regexitize_verse(verse, va_before = False, without_last_space = False):
    "Add regex patterns to list of quranic words"

    # va_pattern1 = "(^" + "و" + " )"
    # va_repl1 = "\\b\\1?"
    # va_pattern2 = "( " + "و" + "$)"
    # va_repl2 = "\\1 ?"
    # va_pattern3 = "( " + "و" + " )"
    # va_repl3 = "\\1?"

    qlist = verse.split(" ")
    regexd_verse = ""
    for ind in range(len(qlist)):
        if 'و' == qlist[ind] and ind == 0 and len(qlist)>1:
            regexd_verse += "\\b" + "و" + " ?"
            va_before = True
        elif 'و' == qlist[ind] and ind == len(qlist)-1:
            #regexd_verse += "و" + " ?"
            va_before = True
            pass
        elif 'و' == qlist[ind]:
            regexd_verse += "\\b" + "و" + " ?"
            va_before = True
        else:
            regexd_verse += ('\\b' if not va_before else '') + qlist[ind] + "\\b "
            va_before = False
    if without_last_space and not va_before:
        regexd_verse = regexd_verse[:-1]
    return regexd_verse, va_before

# def regexitize_quran(qbigram_bag):
#     "Add regex patterns to quran"
#
#     # va_pattern1 = "(^"+"و"+" )"
#     # va_repl1 = R"(?:^| )\1?"
#     # va_pattern2 = "( "+"و"+"$)"
#     # va_repl2 = R"(?:^| )\1 ?"
#     #
#     # oo_pattern = "([^ ]*" + "[^ (?: " + "ذ" + ")( " + "بن" + ")])" + "و" + "( |$)"
#     # oo_repl = R"\1" + "وا?" + R"\2"
#
#     # va_pattern1 = "(^"+"و"+" )"
#     # va_repl1 = R"(?:^| )\1?"
#     # va_pattern2 = "( "+"و"+"$)"
#     # va_repl2 = R"(?:^| )\1 ?"
#     #
#     # oo_pattern = "([^ ]*" + "[^ (?: " + "ذ" + ")( " + "بن" + ")])" + "و" + "( |$)"
#     # oo_repl = R"\1" + "وا?" + R"\2"
#
#     qbigram_bag_keys = list(qbigram_bag.keys())
#     for key in qbigram_bag_keys:
#         new_key = key
#
#         "وا"
#         new_key = re.sub(oo_pattern, oo_repl, new_key)
#
#         "و"
#         new_key = re.sub(va_pattern1, va_repl1, new_key)
#         new_key = re.sub(va_pattern2, va_repl2, new_key)
#
#         "الف کوچک"
#         ######### new_key = f(new_key) ################
#         # it managed but not tested yet
#         # in normalizer
#
#         "ک عربی"
#         ######### new_key = f(new_key) ################
#         # it managed but not tested yet
#         # in normalizer
#
#         qbigram_bag[new_key] = qbigram_bag.pop(key)
#
#     # repl = F"{oo_pattern}" + "اْ" + "?"
#     # for key in qdictionary.keys():
#     #     qdictionary[key] = re.sub(oo_pattern, repl, qdictionary[key])
#
#     # v_pattern = "[ ^]*" + "و" + "[ |$]"
#     # repl = F"{v_pattern}"+"اْ"+"?"
#     # for key in qdictionary.keys():
#     #     qdictionary[key] = re.sub(v_pattern, repl, qdictionary[key])
#     # [(re.findall(".*[^ ]و"+"[ |$]", qdictionary[ki]), ki) for ki in list(qdictionary.keys()) if len(re.findall(".*[^ ]و"+"[ |$]", qdictionary[ki]))!=0]





#input = "   فِی قَوْلِهِ تَعَالَی قُلْ هذِهِ سَبِیلِی أَدْعُوا إِلَی اللهِ عَلی بَصِیرَةٍ أَنَا وَ مَنِ اتَّبَعَنِی قَالَ هِیَ وَلَایَتُنَا أَهْلَ الْبَیْتِ"
#input = " « آ» به شکل ( ٰ ) نوشته می‌شود +++ تبدیل حرفی(ک) به حرفی دیگر(ك)"
#input = "نحوه فارسی(کشــــــــــــیده‌نویسی)"
#input = " قـــــــــــــــــــــــــــــــــــــل هو اللــــه احد تایپ حروف کشــــــــیده شده"
#input = "حضرت موسیٰ (علیٰ نبیّنا و آله و علیه السّلام) به خداوند متعال عرض کرد: رَبَّنا اِنَّکَ ءاتَیتَ فِرعَونَ وَ مَلَاَهُ زینَةً"
#input = "خداوند متعال در جواب فرمود: قالَ قَد اُجیبَت دَعوَتُکُما؛دعای شما دو نفر را -موسیٰ و هارون را- ما قبول کردیم، مستجاب کردیم، امّا شرط دارد: فَاستَقیما وَ لا تَتَّبِعآنِّ سَبیلَ الَّذینَ لا یَعلَمون؛(۲) بِایستید، استقامت کنید. استقامت در میدان جنگ نظامی یک‌جور استً"
#input = "آیه را زیر لب زمزمه می‌کرد که أَمْ حَسِبْتَ أَنَّ أَصْحَبَ ٱلْكَهْفِ وَٱلرَّقِيمِ كَانُواْ مِنْ ءَايَتِنَا عَجَبًا، آیا اصحاب کهف را از عجایب آیات ما می‌پنداری؟ِ"
#input = "رب العالمین همه چیزی خیلی خوب پیش رفت."
input = "دَرَجَاتٍ مِنْهُ وَ مَغْفِرَةً وَرَحْمَةً و َكَانَ اللَّهُ غَفُورًا رَحِيمًا"

test1 = 'قالوا ربنا' #passed
test2 = 'وَهُوَ مِنَ الصَّادِقِينَ' #passed
test3 = 'خالدین فیها' #passed
test4 = 'ان الله غفور' #passed
test5 = 'و العصر' #passed
test6 = 'الله دز آسمان‌ها فقط نیست. در همه جاست.' #passed
test7 = 'حضرت عیس مسیح پیش از پیامبر خاتم به پیامبری رسید'  # passed
test8 = 'خداوند در دل های شکسته قرار دارد' # passed
test9 = 'بسیاری از شاعران ما درباره‌ی طور سینین شعر گفته‌اند.' # passed
test10 = 'پروردگارا ما را به صراط مستقيم هدایت کن' #passed
test11 = 'تو اول بسمِ الله را بگو و بعد کار را شروع کن' #problem
test12 = 'قوم إبراهيم وقوم لوط' #passed
test13 = 'خداوند چگونه در قرآن انسان را مورد خطاب قرار می‌دهد؟' #passed
test14 = 'اگر بدانیم مقصود خداوند از أُولَٰئِكَ هُمُ الْوَارِثُونَ همان ما انسان‌های وارسته هستیم، دیگر بسیاری از رفتارها را تکرار نمی‌کنیم' #passed
test15 = 'سرانجام قوم لوط در قرأن چه شد؟' #passed
test16 = 'وَالَّذِينَ هُمْ عَلَىٰ صَلَوَاتِهِمْ يُحَافِظُونَ' #passed
test17 = 'سرزمین مصر، سرزمین فراعنه' #passed
test18 = 'و آنجا که حضرت یوسف می‌فرماید: مَعَاذَ اللَّهِ ۖ إِنَّهُ رَبِّي أَحْسَنَ مَثْوَايَ ۖ إِنَّهُ لَا يُفْلِحُ الظَّالِمُونَ' #passed
test19 = 'آیا آیه‌ی إِنَّمَا وَلِيُّكُمُ اللَّهُ وَرَسُولُهُ وَالَّذِينَ آمَنُوا الَّذِينَ يُقِيمُونَ الصَّلَاةَ وَيُؤْتُونَ الزَّكَاةَ وَهُمْ رَاكِعُونَ دربارهی حضرت علی (علیه‌السلام) است؟' #passed
test20 = 'شان نزول آیه ی انما وليكم اللّه ورسوله والّذين امنوا' #passed

#input = test20

bad_alphabets = ['\u200c' , 'پ' ,  'ژ' , 'چ' , 'گ']
verbs_needs_alef = ['ملاقو', 'تتلو', 'یتلو', 'یدعو', 'یعفو', 'واولو', 'اولو', 'امرو', 'ویعفو','تبو','اندعو',  'باسطو',  'تبلو', 'اشکو', 'ادعو','لتتلو','یمحو','ندعو','ساتلو', 'یرجو', 'وادعو', 'اتلو','نتلو', 'لتنو','ترجو','مهلکو', 'لیربو','یربو','لتارکو','لذایقو', 'صالو','ویرجو','کاشفو','لیبلو','ونبلو','ونبلو','مرسلو','تدعو','لصالو']
verbs_needs_alef_patt = "("
for el in verbs_needs_alef:
    # if el == 'اولو' or el == 'واولو' :
    #     el = 'واولو(?: الالباب)'
    verbs_needs_alef_patt += F"\\b{el}\\b|"
verbs_needs_alef_patt = verbs_needs_alef_patt[:-1]+")"

num_of_verses = 6236
num_of_suras = 114
Alireza_localrunning = True


"Create quranic dictionary"
if Alireza_localrunning:  
  qtext = codecs.open("../output/v_seperated.txt", 'r', 'utf-8').read()
  #qtext = codecs.open("../Quran Data/id_text_with_orthographies.txt", 'r', 'utf-8').read()
  qtext = qtext.split("\n")
else:
  url = "https://raw.githubusercontent.com/aSafarpoor/open_repo_storhouse_for_nlp_Quran/main/v_seperated.txt"
  response = urllib.request.urlopen(url)
  data = response.read()      # a `bytes` object
  text = data.decode('utf-8')
  qtext = text.split("\n")
qdictionary = {}
num_of_verses = len(qtext)
for i in qtext:
  parted_verse = i.split("\t")
  sura_verse_num = parted_verse[0]
  verse = parted_verse[1]
  qdictionary[sura_verse_num] = verse[:]
qdictionary_keys = list((qdictionary.keys()))



"Normalize quran"
for i in range(num_of_verses):
  sin = qdictionary[qdictionary_keys[i]]
  sout1 = general_normalizer_1(qdictionary[qdictionary_keys[i]])
  sout2 = general_normalizer_2(sout1)
  qdictionary[qdictionary_keys[i]] = sout2

# v_ended_words = []
# v_ended_words_wid_id = []
# for key in qdictionary.keys():
#     for word in qdictionary[key].split(" "):
#         if word[-1] == 'و' and word not in v_ended_words:
#             v_ended_words.append(word)
#             v_ended_words_wid_id.append((key, word))

"Save Normalized output"
# # file = codecs.open("normalized.txt", "w", "utf-8")
# # for i in range(6236):
# #   file.write(qdictionary_keys[i])
# #   file.write("\t")
# #   file.write(qdictionary[qdictionary_keys[i]])
# #   file.write("\n")
# # file.close()


"Add regex patterns to qdictionary"
regexitize_qdictionary(qdictionary)


"Creating token bag"
qbigram_bag = {}
qdictionary_keys = list(qdictionary.keys())
for i in range(len(qdictionary)):
  k_name = qdictionary_keys[i]
  verse = qdictionary[k_name]
  temp = verse.split(" ")
  for j in range(len(temp)-1):
      try:
        qbigram_bag[temp[j] + " " + temp[j+1]].append((k_name,j))
      except:
        qbigram_bag[temp[j] + " " + temp[j+1]] = [(k_name,j)]
      # if temp[j] == 'و' :
      #   try:
      #     qbigram_bag[temp[j]  + temp[j+1]].append((k_name,j))
      #   except:
      #     qbigram_bag[temp[j]  + temp[j+1]] = [(k_name,j)]

#rare_input = input[:]
input_normd = input_normalizer(input)
#print(input_normd)
input_list = input_normd.split(" ")
input_bigram = []
for i in range(len(input_list)-1):
  # flag = True
  # for alphabet in bad_alphabets:
  #   if alphabet in input_list[i] or alphabet in input_list[i+1]:
  #     flag = False
  # if flag:
  #   input_bigram.append(input_list[i] + " " + input_list[i+1])
  if not any(alphabet in input_list[i] or alphabet in input_list[i+1] for alphabet in bad_alphabets):
      input_bigram.append(input_list[i] + " " + input_list[i+1])


"Add regex patterns to qbigram_bag"
regexitize_qbigrambag(qbigram_bag)


"re.find on all the quranic regexitized bigram and input bigram"
qbigram_bag_keys = list(qbigram_bag.keys())
qbigram_found_list = []
# for bigram in input_bigram:
#   if bigram in qbigram_bag_keys:
#     for i in qbigram_bag[bigram]:
#       search_found_list.append([i,bigram])
#qbigram_found_list = [(qbigram, qbigram_bag[qbigram]) for qbigram in qbigram_bag_keys if len(re.findall(qbigram, input_normd))!=0]
qbigram_found_list = []
for qbigram in qbigram_bag_keys:
    pattern = qbigram
    # pattern.replace(" ", "\\b \\b")
    # pattern = "\\b"+pattern+"\\b"
    if len(re.findall(pattern, input_normd))!=0:
        qbigram_found_list.append((qbigram, qbigram_bag[qbigram]))
        #qbigram_found_list.extend([[id, qbigram] for id in qbigram_bag[qbigram]])

    # matches = list(re.findall(qbigram, input_normd))
    # if len(matches) != 0:
    #     for match in matches:
    #         for id in qbigram_bag[qbigram]:
    #             verse = qdictionary[id]
    #             rules = rule_maker(verse, qbigram, start, end)

output_bag = []
for qbigram in qbigram_found_list:
    for inner_tup in qbigram[1]:
        id, index = inner_tup
        qbigram_text = qbigram[0]
        verse = qdictionary[id]
        #rules = rule_maker(verse ,qbigram[0], tup[1])
        rule = rule_maker(verse ,qbigram_text, index)
        #for rule in rules:
        matches = list(re.finditer(rule,input_normd))
        # if isinstance(matches, list):
        #     temp = ""
        #     for match in matches:
        #         if(len(match) > len(temp)):
        #             temp = sample[:]
        #         output_bag.append([tup[0],temp])
        # elif isinstance(subout, str):
        if len(matches)!=0:
            for match in matches:
                res = F"{input_normd[match.regs[0][0]:match.regs[0][1]].strip()} {id}"
                if res not in output_bag:
                    output_bag.append(res)
for x in output_bag:
    print(x)
# output_bag.sort()
# new_list = [output_bag[0]]
# for i in range(1,len(output_bag)):
#   if output_bag[i] == new_list[-1] :
#     pass
#   elif output_bag[i][0] == new_list[-1][0]:
#     if new_list[-1][0] in output_bag[i][1]:
#       new_list[-1] = output_bag[i]
#   else:
#       new_list.append(output_bag[i])

